# How Many Replicators Does it Take to Achieve Reliability

Part of the Crowdsourced Replication Initiative ([Breznau, Rinke and Wuttke et al 2018](https://osf.io/preprints/socarxiv/6j9qb/)) investigating the reliability of research. Includes a direct replication of Brady and Finnigan ([2014](https://journals.sagepub.com/doi/full/10.1177/0003122413513022)) undertaken by 85 independent research teams to verify their original numerical results testing the hypothesis that immigration undermines support for social policy in rich/Global North democracies. There was an experimental condition varying the transparency of the replication materials provided. 


## Paper Abstract

The paper reports findings from a crowdsourced replication. Eighty-five replicator teams attempted to verify numerical results reported in an original study of policy preferences and immigration by running the same models with the same data. The replication involved an experimental condition. A randomly assigned transparent group received the original study and code, while an opaque group received only a methods section, rough results description and no code. The transparent group mostly verified the original study with the same sign and p-value threshold (95.7%), while the opaque group had less success (89.3%). Exact numerical reproductions to the second decimal place were far less common (76.9% and 48.1%), and the number of teams who verified at least 95% of all effects in all models they ran was 79.5% and 65.2%. These results suggest that a single replication is moderately reliable, and only when the original study is highly transparent. Depending on how we define reliability, it would in most cases take a minimum of three independent replications to achieve a high level of confidence. Qualitative investigation of the replicators’ workflows reveals many causes of error including mistakes and procedural variations. This study quantifies inter-researcher reliability. Although minor error across researchers is not surprising, we show this occurs where it is least expected. Even when we curate the results to boost ecological validity, the error remains large enough to undermine reliability between-researchers to some extent. The may explain the current “reliability crisis” in the social sciences, if not beyond, because most forms of research involve much more decision-making that theoretically introduces even more inter-researcher reliability concerns. The obvious implication is greater transparency, something sociology has been hesitant to embrace. Wider implications are greater attention to detail, using technology to improve our research and a greater degree of humility when reporting results.

## User Notes

The workflow is captured in the folder 'codes' with Rmd files that should be run in order starting with 00 through 02. These files have been run and knitted into html output which can be read in any browser. More information about the original data files are available in each code file. To see these open the [code](../code/) folder or follow the following links:

| File | Description |
| [00_Qualitative_Coding.Rmd](../code/00_Qualitative_Coding.html) | Extracts and counts qualitative categorizations of error by team |
| [01_Data_Prep.Rmd](../code/01_Data_Prep.html) | Prepares and combines participant survey results and replicated numerical effects by team |
| [02_Analysis.Rmd](../code/02_Analysis.html) | Correlations and regressions predicting our three reliability measures and two qualitative categories |
| [03_Figures.Rmd](../code/03_Figures.html) | Produces figures and additional tables |
