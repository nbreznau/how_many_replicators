---
title: "Researcher Variability in Replications: Replication File, Step 2"
output: html_document
---

|Metric|Definition|Measure|
|------|----------|---------|
|*Verification* (verif)| The direction of the regression coefficient is the same as the original & either within 0.05 absolute difference or the same significance at p<0.05 threshold|1=verified, 0=not|
|*Exact Replication* (exact)|The value of the replication odd-ratio is identical to the second decimal place of the original|1=exact, 0=not|
|*Replication Error* (deviance_abs)|The absolute deviation of the replicated odd-ratio from the original odds-ratio | continuous measure starting from exact (=0) and increasing in positive values to measure the error |

```{r setup, warning = F, message = F}
rm(list = ls())
library(pacman)


pacman::p_load("dplyr", "readr", "lattice", "tidyr", "readxl", "knitr", "boot", "ragg", "kableExtra", "ggpubr","lme4", "jtools","sjPlot", "sjmisc", "sjlabelled", "rvest", "lavaan", "lavaanPlot", "see")


```


```{r setup2, warning = F, message = F}
# load data from 01_Data_Prep
load(file = "data/data.Rdata")


# disable scientific notation
options(scipen = 999)
```


A total of `r sum(cri_long$insamp, na.rm = T)` results from `r length(unique(cri_long$u_teamid))` teams

## Figure 1

This plots how many replicators it would take to achieve replication reliability defined as the minimum number of replicators to result in a majority coming to the correct answer (here a verification) at a confidence level of 95%. 

We calculate *P* as the probability that X=x (that a replicator will get the correct answer more than 50% of the time; for simplicity we assign 51% to x)

$P(X=x)=(\frac{n}{x}) p^x(1-p)^{(n-x)}$

Then we calculate n for different values of P and p. 

First we need to map values of *x* for *n* trials (so that *x* is always at least 51% of n). We do this in a matrix to make plotting of the figure easy.

```{r fig1}
# calculate x at its for each value of n
# use a simple 25 n by 25 x matrix

fig1 <- data.frame(n = rep(1:25,25))
fig1 <- data.frame(fig1[order(fig1$n),])
colnames(fig1) <- c("n")
fig1$x <- rep(1:25,25)

fig1 <- fig1 %>%
  mutate(x_t = round(n/1.99,0),
         x_min = ifelse(n/x_t == 2, x_t + 1, x_t),
         x = ifelse(x < x_min, x_min, x), # remove cases where x is less than 51%
         p_75 = 1-pbinom(x-1,n,0.75)) # compute cumulative probability of all values less than x, then subtract this from 1 to get all values greater than or equal to x

#loop to generate columns for remaining values of p
for (p in 76:99) {
fig1 <- fig1 %>%
  mutate(hold = 1-pbinom(x-1,n,p/100))

assign(paste0("p_",p), data.frame(fig1$hold))

}

ps <- cbind(p_76,p_77,p_78,p_79,p_80,p_81,p_82,p_83,p_84,p_85,p_86,p_87,p_88,p_89,p_90,p_91,p_92,p_93,p_94,p_95,p_96,p_97,p_98,p_99)

colnames(ps) <- c("p_76","p_77","p_78","p_79","p_80","p_81","p_82","p_83","p_84","p_85","p_86","p_87","p_88","p_89","p_90","p_91","p_92","p_93","p_94","p_95","p_96","p_97","p_98","p_99")

fig1 <- cbind(fig1,ps)

rm(p_76,p_77,p_78,p_79,p_80,p_81,p_82,p_83,p_84,p_85,p_86,p_87,p_88,p_89,p_90,p_91,p_92,p_93,p_94,p_95,p_96,p_97,p_98,p_99,ps,p)

fig1 <- fig1[!(fig1$x > fig1$n),] #remove rows where x is greater than n         

fig1 <- fig1 %>%
  select(-hold)

# now create a df for plotting with the minimum n for each threshold of P
fig1_plot <- data.frame(matrix(nrow=25,ncol=4))
colnames(fig1_plot) <- c("P_90","P_95","P_99","p")


i = 1
for (col in c("p_75","p_76","p_77","p_78","p_79","p_80","p_81","p_82","p_83","p_84","p_85","p_86","p_87","p_88","p_89","p_90","p_91","p_92","p_93","p_94","p_95","p_96","p_97","p_98","p_99")) {
  figx <- fig1 %>%
  select(n, col) 
  colnames(figx) <- c("n","p")
  fig1_plot[i,1] <- min(figx$n[figx$p > 0.90])
  fig1_plot[i,2] <- min(figx$n[figx$p > 0.95])
  fig1_plot[i,3] <- min(figx$n[figx$p > 0.99])
  i = i + 1
} 

fig1_plot$p <- seq(75,99)

```

## Figure 1. Simulated Replication Reliabilities

```{r fig1}

```


```{r fig1x}



plot1 <- ggplot(cri_long, aes(x = count, y = deviance_abs, color = Exp1, shape = Exp1)) +
  geom_point(size = 2.5) + 
  coord_cartesian(ylim=c(0, 0.5)) +
  ylab("Replicated Effect\n(deviance from original)") +
  scale_color_manual(values = c("#009E73","#D55E00"), name = "Replication Group", labels = c("Transparent","Opaque")) +
  scale_shape_manual(values = c(6,2)) +
  theme(axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_text(size = 12, color = "black"),
        axis.line.x = element_line(),
        axis.line.y = element_line(),
        legend.position = "none",
        plot.background = element_blank(),
        panel.background = element_blank(),
        legend.background = element_blank(),
        legend.key = element_blank(),
        legend.text = element_text(size = 11)) +
  guides(color = guide_legend(override.aes = list(shape = c(19,19), size=5, fill = NA)), shape = F)


# get means to add
mean0 <- round(mean(cri_long$deviance_abs[cri_long$Exp1 == 0], na.rm = T),3)
mean1 <- round(mean(cri_long$deviance_abs[cri_long$Exp1 == 1], na.rm = T),3)

plot2 <- ggboxplot(cri_long, "Exp1","deviance_abs", color = "Exp1", 
                   whisklty = 0, palette = c("#009E73","#D55E00"), 
                   outlier.shape = NA, size = 1) +
  coord_cartesian(ylim=c(0, 0.04)) +
  xlab("Group") +
  scale_fill_discrete(name = "Replication Group", labels = c("Transparent","Opaque")) +
  annotate(geom="text", x=1.1, y=0.0145, label = paste0("mean"),
              color="#009E73", size = 3, hjust = 0) + 
  annotate(geom="text", x=2.1, y=0.036, label = paste0("mean"),
              color="#D55E00", size = 3, hjust = 0) +
  annotate(geom="text", x=1.1, y=0.013, label = paste0("deviance"),
              color="#009E73", size = 3, hjust = 0) + 
  annotate(geom="text", x=2.1, y=0.0345, label = paste0("deviance"),
              color="#D55E00", size = 3, hjust = 0) +
  annotate(geom="text", x=1.1, y=0.011, label = paste0(mean1),
              color="#009E73", size = 3, hjust = 0, fontface = 2) + 
  annotate(geom="text", x=2.1, y=0.0325, label = paste0(mean0),
              color="#D55E00", size = 3, hjust = 0, fontface = 2) +
  theme(axis.title.y = element_blank(),
        legend.position = "bottom",
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_text(size = 12),
        plot.margin = margin(2,2,2,2),
        legend.text = element_text(size = 11))

plotF <- ggarrange(plot1,NA,plot2, ncol = 3, widths = c(7,1,7), common.legend = T, legend = "bottom")
agg_png(filename = "results/Fig1.png", res = 144, width = 900)
plotF
dev.off()

knitr::include_graphics("results/Fig1.png")
```
## Table 2

### Setup

```{r table2}

# start with descriptive table

desc <- as.data.frame(matrix(nrow = 18, ncol = 11))

desc[1,] <- c("","","Means by Sample","","","Pear. Correlations w/ Raw Results","","","","","")

desc[2,] <- c("Variables","Measurement","Transparent","Opaque", "Pooled", "Verification", "Exact Verif.", "Deviance", "", "", "")

desc[,1] <- c("", "Variables","Raw Replication Results", "Verification", "Exact Verification", "Deviance", "Curated Replication Results", "Verification", "Exact Verification", "Deviance", "Independent Variables", "Stata", "Sociology Degree", "Stats-Skill", "Difficulty", "Team Size", "Routine", "Transparent")

desc[,2] <- c("", "Measurement","", "same direction =1","identical at two decimals =1", "absolute difference with original","", "same direction =1","identical at two decimals =1", "absolute difference with original","", "other software =0","other degrees =0","4-question scale, stdzd,","1-question, stdzd.","1-3 persons", "routine cause =1", "Transparent group =1")

# fill in transparent group results

desc[3:18,3] <- c("", 
                  mean(cri_long$verif[cri_long$Exp1 == 1], na.rm = T),
                  mean(cri_long$exact[cri_long$Exp1 == 1], na.rm = T),
                  mean(cri_long$deviance_abs[cri_long$Exp1 == 1], na.rm = T),
                  "",
                  mean(cri_cur_long$verif[cri_cur_long$Exp1 == 1], na.rm = T),
                  mean(cri_cur_long$exact[cri_cur_long$Exp1 == 1], na.rm = T),
                  mean(cri_cur_long$deviance_abs[cri_cur_long$Exp1 == 1], na.rm = T),
                  "",
                  mean(cri_long$stata[cri_long$Exp1 == 1], na.rm = T),
                  mean(cri_long$degree_soc[cri_long$Exp1 == 1], na.rm = T),
                  mean(cri_long$stat_skill[cri_long$Exp1 == 1], na.rm = T),
                  mean(cri_long$difficult[cri_long$Exp1 == 1], na.rm = T),
                  mean(cri_long$numinteam[cri_long$Exp1 == 1], na.rm = T),
                  mean(cri_long$routine[cri_long$Exp1 == 1], na.rm = T),
                  mean(as.numeric(cri_long$Exp1[cri_long$Exp1 == 1]), na.rm = T)
                  )

# opaque

desc[3:18,4] <- c("", 
                  mean(cri_long$verif[cri_long$Exp1 == 0], na.rm = T),
                  mean(cri_long$exact[cri_long$Exp1 == 0], na.rm = T),
                  mean(cri_long$deviance_abs[cri_long$Exp1 == 0], na.rm = T),
                  "",
                  mean(cri_cur_long$verif[cri_cur_long$Exp1 == 0], na.rm = T),
                  mean(cri_cur_long$exact[cri_cur_long$Exp1 == 0], na.rm = T),
                  mean(cri_cur_long$deviance_abs[cri_cur_long$Exp1 == 0], na.rm = T),
                  "",
                  mean(cri_long$stata[cri_long$Exp1 == 0], na.rm = T),
                  mean(cri_long$degree_soc[cri_long$Exp1 == 0], na.rm = T),
                  mean(cri_long$stat_skill[cri_long$Exp1 == 0], na.rm = T),
                  mean(cri_long$difficult[cri_long$Exp1 == 0], na.rm = T),
                  mean(cri_long$numinteam[cri_long$Exp1 == 0], na.rm = T),
                  mean(cri_long$routine[cri_long$Exp1 == 0], na.rm = T),
                  mean(as.numeric(cri_long$Exp1[cri_long$Exp1 == 0]), na.rm = T)
                  )

# pooled

desc[3:18,5] <- c("", 
                  mean(cri_long$verif, na.rm = T),
                  mean(cri_long$exact, na.rm = T),
                  mean(cri_long$deviance_abs, na.rm = T),
                  "",
                  mean(cri_cur_long$verif, na.rm = T),
                  mean(cri_cur_long$exact, na.rm = T),
                  mean(cri_cur_long$deviance_abs, na.rm = T),
                  "",
                  mean(cri_long$stata, na.rm = T),
                  mean(cri_long$degree_soc, na.rm = T),
                  mean(cri_long$stat_skill, na.rm = T),
                  mean(cri_long$difficult, na.rm = T),
                  mean(cri_long$numinteam, na.rm = T),
                  mean(cri_long$routine, na.rm = T),
                  mean(as.numeric(cri_long$u_expgroup1), na.rm = T))




```

### Make Table

```{r tbl2corr, warning = F, message = F}
cri_long <- cri_long %>%
  ungroup()
cri_cur_long <- cri_cur_long %>%
  ungroup()

desc_c1 <- select(cri_long, verif, exact, deviance_abs, stata, degree_soc, stat_skill, difficult, numinteam, routine, u_expgroup1)

desc_c1$u_expgroup1 <- as.numeric(desc_c1$u_expgroup1)

cor1 <- as.data.frame(cor(desc_c1, use = "pairwise"))

desc_c2 <- select(cri_cur_long, verif, exact, deviance_abs, stata, degree_soc, stat_skill, difficult, numinteam, routine, u_expgroup1)



desc_c2$u_expgroup1 <- as.numeric(desc_c2$u_expgroup1)

cor2 <- as.data.frame(cor(desc_c2, use = "pairwise"))

# test for sig of corrs

test1 <- psych::corr.test(desc_c1)
test2 <- psych::corr.test(desc_c2)

test1df <- as.data.frame(round(test1[["p"]],3))[4:10,1:3]
test2df <- as.data.frame(round(test2[["p"]],3))[4:10,1:3]

# we drop 'other' as a degree because it does not have enough of any one degree type to be meaningful, and thus we just compare sociology and political science

desc_c1a <- subset(cri_long, degree != 3, select = c(verif, exact, deviance_abs, degree))
desc_c2a <- subset(cri_cur_long, degree != 3, select = c(verif, exact, deviance_abs, degree))

test1a <- psych::corr.test(desc_c1a)
test2a <- psych::corr.test(desc_c2a)

test1adfr <- as.data.frame(round(test1a[["r"]],3))[4,1:3]
test2adfr <- as.data.frame(round(test2a[["r"]],3))[4,1:3]

test1adfp <- as.data.frame(round(test1a[["p"]],3))[4,1:3]
test2adfp <- as.data.frame(round(test2a[["p"]],3))[4,1:3]

# update p value table with adjusted scores for degree

test1df[2,1:3] <- test1adfp
test2df[2,1:3] <- test2adfp

# many teams made both routine and non-routine mistakes, these should not be counted as "routine" as mistakes are generally far more serious in their impact on results. Therefore, recode teams that had non-routine errors as well that could not be counterfactual corrected.

desc_c1b <- select(cri_long, verif, exact, deviance_abs, routine, u_teamid)
desc_c2b <- select(cri_cur_long, verif, exact, deviance_abs, routine, u_teamid)

desc_c1b <- desc_c1b %>%
  mutate(routine = ifelse(u_teamid == 28 | u_teamid == 33 | u_teamid == 70 | u_teamid == 101 , NA, routine))
desc_c1b <- desc_c1b %>%
  mutate(routine = ifelse(u_teamid == 60, 1, routine)) %>%
  select(-c(u_teamid))

desc_c2b <- desc_c2b %>%
  mutate(routine = ifelse(u_teamid == 28 | u_teamid == 33 | u_teamid == 70 | u_teamid == 101 , NA, routine))
desc_c2b <- desc_c2b %>%
  mutate(routine = ifelse(u_teamid == 60, 1, routine)) %>%
  select(-c(u_teamid))

test1b <- psych::corr.test(desc_c1b)
test2b <- psych::corr.test(desc_c2b)

test1bdfr <- as.data.frame(round(test1b[["r"]],3))[4,1:3]
test2bdfr <- as.data.frame(round(test2b[["r"]],3))[4,1:3]

test1bdfp <- as.data.frame(round(test1b[["p"]],3))[4,1:3]
test2bdfp <- as.data.frame(round(test2b[["p"]],3))[4,1:3]

# update p value table with adjusted scores for routine

test1df[6,1:3] <- test1bdfp
test2df[6,1:3] <- test2bdfp

# add correlations to table

desc[4:6,6:8] <- cor1[1:3,1:3]

desc[12:18,6:8] <- cor1[4:10,1:3]

desc[8:10,9:11] <- cor2[1:3,1:3]

desc[12:18,9:11] <- cor2[4:10,1:3]

# add adjusted degree and routine correlations
desc[13,6:8] <- test1adfr
desc[13,9:11] <- test2adfr

desc[17,6:8] <- test1bdfr
desc[17,9:11] <- test2bdfr

write.csv(desc, file = "results/Tbl2.csv")

# remove factor
cri_long$Exp1 <- as.numeric(cri_long$Exp1)
cri_cur_long$Exp1 <- as.numeric(cri_cur_long$Exp1)

cri_agg <- aggregate(cri_long, by = list(cri_long$u_teamid), FUN = mean)
cri_cur_agg <- aggregate(cri_cur_long, by = list(cri_cur_long$u_teamid), FUN = mean)

rm(test1, test2, test1a, test2a, test1df, test2df, test1adfp, test2adfp, test1adfr, test2adfr, test1bdfp, test2bdfp, test1bdfr, test2bdfr)
```


## Explaining Variance

Our observations take place only at the team level, therefore it is only necessary to run a regression on the team averages ('level-2') of the three outcome variables *verif*, *exact* and *deviance_abs*.

Aggregate data to team-level

```{r aggregate_team}
# many teams made both routine and non-routine mistakes, these should not be counted as "routine" as mistakes are generally far more serious in their impact on results. Therefore, recode teams that had non-routine errors as well that could not be counterfactual corrected.

cri_long <- cri_long %>%
  mutate(routine = ifelse(u_teamid == 28 | u_teamid == 33 | u_teamid == 70 | u_teamid == 101 , NA, routine))
cri_long <- cri_long %>%
  mutate(routine = ifelse(u_teamid == 60, 1, routine))

cri_cur_long <- cri_cur_long %>%
  mutate(routine = ifelse(u_teamid == 28 | u_teamid == 33 | u_teamid == 70 | u_teamid == 101 , NA, routine))
cri_cur_long <- cri_cur_long %>%
  mutate(routine = ifelse(u_teamid == 60, 1, routine))


# curated
cri_team <- cri_cur_long %>%
  mutate(u_expgroup1 = as.numeric(u_expgroup1)) %>%
  group_by(u_teamid2) %>%
  summarize(verif_m = mean(verif, na.rm = T),
            verif_sd = sd(verif, na.rm = T),
            exact_m = mean(exact, na.rm = T),
            exact_sd = sd(exact, na.rm = T),
            dev_m = mean(deviance_abs, na.rm = T),
            dev_sd = sd(deviance_abs, na.rm = T),
            stata = mean(stata, na.rm = T),
            degree = mean(degree, na.rm = T),
            stat_skill = mean(stat_skill, na.rm = T),
            numinteam = mean(numinteam, na.rm = T),
            exp = mean(u_expgroup1, na.rm = T),
            difficult = mean(difficult, na.rm = T),
            routine = mean(routine, na.rm = T))

cri_team_orig <- cri_long %>%
  group_by(u_teamid2) %>%
  mutate(u_expgroup1 = as.numeric(u_expgroup1)) %>%
  summarize(verif_m = mean(verif, na.rm = T),
            verif_sd = sd(verif, na.rm = T),
            exact_m = mean(exact, na.rm = T),
            exact_sd = sd(exact, na.rm = T),
            dev_m = mean(deviance_abs, na.rm = T),
            dev_sd = sd(deviance_abs, na.rm = T),
            stata = mean(stata, na.rm = T),
            degree = mean(degree, na.rm = T),
            stat_skill = mean(stat_skill, na.rm = T),
            numinteam = mean(numinteam, na.rm = T),
            exp = mean(u_expgroup1, na.rm = T),
            difficult = mean(difficult, na.rm = T),
            routine = mean(routine, na.rm = T))

cri_team$degree <- factor(cri_team$degree, labels = c("Sociology", "Political Science", "Other"))
cri_team_orig$degree <- factor(cri_team_orig$degree, labels = c("Sociology", "Political Science", "Other"))

# trim outlier
cri_team$stat_skill <- ifelse(cri_team$stat_skill < -4, -4, cri_team$stat_skill)
cri_team_orig$stat_skill <- ifelse(cri_team_orig$stat_skill < -4, -4, cri_team_orig$stat_skill)




```
Variance decomposition

```{r decomp}
# curated
mlm01_verif <- glmer(verif ~ (1 | u_teamid2), family = binomial, data = cri_cur_long)

mlm11_exact <- glmer(exact ~ (1 | u_teamid2), family = binomial, data = cri_cur_long)

mlm21_deviance <- lmer(deviance_abs ~ (1 | u_teamid2), data = cri_cur_long)

# original
mlm01_verif_orig <- glmer(verif ~ (1 | u_teamid2), family = binomial, data = cri_long)

mlm11_exact_orig <- glmer(exact ~ (1 | u_teamid2), family = binomial, data = cri_long)

mlm21_deviance_orig <- lmer(deviance_abs ~ (1 | u_teamid2), data = cri_long)

tab_model(mlm01_verif, mlm01_verif_orig, mlm11_exact, mlm11_exact_orig, mlm21_deviance, mlm21_deviance_orig,show.ci = F, file = "results/mlm.htm")

mlm <- as.data.frame(read_html("results/mlm.htm") %>% html_table(fill=TRUE))

kable_styling(kable(mlm))
  
```

Variance decomposition reveals that of the total variance, `r paste0(100*(as.numeric(mlm$verif[mlm$Var.1 == "ICC"])),"%")` of *Verification*, `r paste0(100*(as.numeric(mlm$exact[mlm$Var.1 == "ICC"])),"%")` of *Exact Verification*, and `r paste0(100*(as.numeric(mlm$deviance_abs[mlm$Var.1 == "ICC"])),"%")` of *Replication Deviance* occurs between-teams (as opposed to within-teams).




### Variance Plots

```{r var_plot}
cri_cur_long <- cri_cur_long %>%
  mutate(degreeF = as.factor(degree),
         stataF = as.factor(stata),
         stat_skillF = as.factor(ifelse(stat_skill > 0, 1, 0)),
         dev_abs_log = log(deviance_abs+0.001),
         dev_abs_min = min(dev_abs_log, na.rm = T),
         dev_abs_log_c = dev_abs_log - dev_abs_min)

# alt summary stats by group
cri_sum_plot <- cri_cur_long %>%
  group_by(degree) %>%
  summarise(dev_degree = mean(deviance_abs, na.rm = T),
            dev_degree_sd = sd(deviance_abs, na.rm = T),
            dev_degree_n = n())

cri_sum_plot2 <- cri_cur_long %>%
  group_by(stataF, u_expgroup1) %>%
  summarise(dev_stataex = mean(deviance_abs, na.rm = T),
            dev_stataex_sd = sd(deviance_abs, na.rm = T),
            dev_stataex_n = n())  

cri_sum_plot3 <- cri_cur_long %>%
  group_by(stat_skillF) %>%
  summarise(dev_stat = mean(deviance_abs, na.rm = T),
            dev_stat_sd = sd(deviance_abs, na.rm = T),
            dev_stat_n = n())

# routine errors
cri_sum_plot4 <- cri_cur_long %>%
  group_by(routine, u_expgroup1) %>%
  summarise(dev_routine = mean(deviance_abs, na.rm = T),
            dev_routine_sd = sd(deviance_abs, na.rm = T),
            dev_routine_n = n())

cri_sum_plot4 <- cri_sum_plot4[1:4,]



cri_sum_plot$degree <- as.factor(cri_sum_plot$degree)
cri_sum_plot2$stataF <- as.factor(cri_sum_plot2$stataF)
cri_sum_plot3$stat_skillF <- as.factor(cri_sum_plot3$stat_skillF)

# reorder for 4-category plots
cri_sum_plot2[,1] <- as.factor(c(1,3,2,4))
cri_sum_plot4[,1] <- as.factor(c(1,3,2,4))

```



```{r bar1}
ggplot(cri_sum_plot) +
  geom_bar(aes(y=dev_degree, x=degree, fill=degree), stat = 'identity') +
  geom_errorbar(aes(x=degree, ymin=dev_degree-(dev_degree_sd/sqrt(dev_degree_n)), ymax = dev_degree+(dev_degree_sd/sqrt(dev_degree_n)), fill=degree)) +
  coord_flip() +
  scale_fill_manual(values = c("#D55E00","#009E73","goldenrod")) +
  annotate(geom="text", x=2.8, y=0.001, label = paste0("Other"),
              color="black", size = 4, hjust = 0) +
  annotate(geom="text", x=1.8, y=0.001, label = paste0("Political Science"),
              color="black", size = 4, hjust = 0) +
  annotate(geom="text", x=0.8, y=0.001, label = paste0("Sociology"),
              color="black", size = 4, hjust = 0) +
  ylab(label = "Deviance from Original Results") +
  theme_classic() +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank()
  )
```


```{r bar2}

agg_png(filename = "results/Fig2.png", width = 800, height = 600, res = 144)
ggplot(cri_sum_plot2) +
  geom_bar(aes(y=dev_stataex, x=stataF, fill=stataF), stat = 'identity') +
  geom_errorbar(aes(x=stataF, ymin=dev_stataex-(dev_stataex_sd/sqrt(dev_stataex_n)), ymax = dev_stataex+(dev_stataex_sd/sqrt(dev_stataex_n)), width = 0.4)) +
  coord_flip() +
  scale_fill_manual(values = c("#D55E00","#009E73","goldenrod","mediumturquoise")) +
  geom_label(aes(x=3.8, y=0.0005, label = "Stata"), fill = "white",
              color="black", label.size = NA, hjust = 0, label.r = unit(0, "lines")) +
  geom_label(aes(x=2.8, y=0.0005, label = "Other"), fill = "white",
              color="black", label.size = NA, hjust = 0, label.r = unit(0, "lines")) +
  geom_label(aes(x=1.8, y=0.0005, label = "Stata"), fill = "white",
              color="black", label.size = NA, hjust = 0, label.r = unit(0, "lines")) +
  geom_label(aes(x=0.8, y=0.0005, label = "Other"), fill = "white",
              color="black", label.size = NA, hjust = 0, label.r = unit(0, "lines")) +
  annotate(geom = "text", label = "Transparent Group", y = -0.001, x = 3.5, angle = 90, vjust = 0) +
  annotate(geom = "text", label = "Opaque Group", y = -0.001, x = 1.5, angle = 90, vjust = 0) +
  geom_vline(xintercept = 2.5, color = "grey23") +
  ylab(label = "Deviance from Original Results") +
  theme_classic() +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank()
  )
dev.off()

knitr::include_graphics("results/Fig2.png")
```

A total of `r length(unique(cri_long$u_teamid[cri_long$routine == 1])) + 4` teams had routine searcher variability, but 4 of these teams also had mistakes that could not be counterfactually repaired. Thus, not counting these 4 teams, `r  round((100*(length(unique(cri_long$u_teamid[cri_long$routine == 1])))/(length(unique(cri_long$u_teamid))-4),1)` percent of the CRI teams had routine researcher variability, but this was far more common in the opaque group which had `r  round((100*(length(unique(cri_long$u_teamid[cri_long$routine == 1 & cri_long$u_expgroup1 == "0"])))/(length(unique(cri_long$u_teamid[cri_long$u_expgroup1 == "0"]))-4)),1)` percent of teams with routine variability compared to only `r  round((100*(length(unique(cri_long$u_teamid[cri_long$routine == 1 & cri_long$u_expgroup1 == "1"])))/(length(unique(cri_long$u_teamid[cri_long$u_expgroup1 == "1"]))-4)),1)` percent in the transparent group.

Those who had no routine error in the transparent group had an exact replication rate of `r  round(100*mean(cri_cur_long$exact[cri_cur_long$routine == 0 & cri_cur_long$u_expgroup1 == "1"], na.rm = T),1)`
```{r bar4}


agg_png(filename = "results/Fig3.png", width = 800, height = 600, res = 144)
ggplot(cri_sum_plot4) +
  geom_bar(aes(y=dev_routine, x=routine, fill=routine), stat = 'identity') +
  geom_errorbar(aes(x=routine, ymin=dev_routine-(dev_routine_sd/sqrt(dev_routine_n)), ymax = dev_routine+(dev_routine_sd/sqrt(dev_routine_n)), width = 0.4)) +
  coord_flip() +
  scale_fill_manual(values = c("#D55E00","#009E73","goldenrod","mediumturquoise")) +
  geom_label(aes(x=4, y=0.0007, label = "Routine Errors in\n34% of Group"), fill = "grey90", fontface = 3, size = 3,
              color="black", label.size = NA, hjust = 0, label.r = unit(0, "lines")) +
  geom_label(aes(x=2, y=0.0007, label = "Routine Errors in\n88% of Group"), fill = "grey90", fontface = 3, size = 3,
              color="black", label.size = NA, hjust = 0, label.r = unit(0, "lines")) +
  geom_label(aes(x=3, y=0.005, label = "No Routine Errors in\n66% of Group"), fill = "white", fontface = 3, size = 3,
              color="black", label.size = NA, hjust = 0, label.r = unit(0, "lines")) +
  geom_label(aes(x=1, y=0.0007, label = "No Routine Errors in\n12% of Group"), fill = "grey90", fontface = 3, size = 3,
              color="black", label.size = NA, hjust = 0, label.r = unit(0, "lines")) +
  annotate(geom = "text", label = "Transparent Group", y = -0.001, x = 3.5, angle = 90, vjust= 0) +
  annotate(geom = "text", label = "Opaque Group", y = -0.001, x = 1.5, angle = 90, vjust = 0) +
  #annotate("text", label = "34%", y = 0.03, x = 4, fontface = 4) +
  geom_vline(xintercept = 2.5, color = "grey23") +
  ylab(label = "Deviance from Original Results") +
  theme_classic() +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank()
  )
dev.off()

knitr::include_graphics("results/Fig3.png")
```





```{r var_plot4}
ggplot(cri_cur_long, aes(y=deviance_abs, x=stat_skillF, fill=stat_skillF)) +
  coord_flip(ylim = c(0,0.02)) +
  #coord_cartesian(ylim = c(0,0.15)) +
  geom_violinhalf() +
  theme_classic() +
  ylab(label = "Replicated Effect Deviance (logged)") +
  scale_fill_manual(values = c("#D55E00","#009E73")) +
  annotate(geom="text", x=1.9, y=0.001, label = paste0("Less Stats Skills/Experience"),
              color="#009E73", size = 4, hjust = 0) +
  annotate(geom="text", x=0.9, y=0.001, label = paste0("More Stats Skills/Experience"),
              color="#D55E00", size = 4, hjust = 0) +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank()
  )

```

These regressions are interesting

### Regressions predicting within-team variance **curated**

```{r varexp}


# variables, researcher aspects

# MEANS
m01_verif_mean <- lm(verif_m ~ exp + stata + stat_skill , data = cri_team)

m11_exact_mean <- lm(exact_m ~ exp + stata + stat_skill , data = cri_team)

m21_deviance_mean <- lm(dev_m ~ exp + stata + stat_skill , data = cri_team)

#SDs
m001_verif_sd <- lm(verif_sd ~ exp + stata + stat_skill , data = cri_team)

m011_exact_sd <- lm(exact_sd ~ exp + stata + stat_skill , data = cri_team)

m021_deviance_sd <- lm(dev_sd ~ exp + stata + stat_skill , data = cri_team)


# unimportant criteria

# MEANS
m02_verif_mean <- lm(verif_m ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team)

m12_exact_mean <- lm(exact_m ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team)

m22_deviance_mean <- lm(dev_m ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team)

#SDs
m002_verif_sd <- lm(verif_sd ~ exp + stata + stat_skill  + difficult, data = cri_team)

m012_exact_sd <- lm(exact_sd ~ exp + stata + stat_skill  + difficult, data = cri_team)

m022_deviance_sd <- lm(dev_sd ~ exp + stata + stat_skill  + difficult, data = cri_team)


# difficulty

# MEANS
m03_verif_mean <- lm(verif_m ~ exp + stata + stat_skill + degree + numinteam + difficult + exp, data = cri_team)

m13_exact_mean <- lm(exact_m ~ exp + stata + stat_skill + degree + numinteam + difficult + exp, data = cri_team)

m23_deviance_mean <- lm(dev_m ~ exp + stata + stat_skill + degree + numinteam + difficult + exp, data = cri_team)

#SDs
m003_verif_sd <- lm(verif_sd ~ exp + stata + stat_skill  + difficult + exp, data = cri_team)

m013_exact_sd <- lm(exact_sd ~ exp + stata + stat_skill  + difficult + exp, data = cri_team)

m023_deviance_sd <- lm(dev_sd ~ exp + stata + stat_skill  + difficult + exp, data = cri_team)



```

### Regressions predicting within-team variance **original**

```{r varexp_orig}


# variables, researcher aspects

# MEANS
m01_verif_mean_orig <- lm(verif_m ~ exp + stata + stat_skill , data = cri_team_orig)

m11_exact_mean_orig <- lm(exact_m ~ exp + stata + stat_skill , data = cri_team_orig)

m21_deviance_mean_orig <- lm(dev_m ~ exp + stata + stat_skill , data = cri_team_orig)

#SDs
m001_verif_sd_orig <- lm(verif_sd ~ exp + stata + stat_skill , data = cri_team_orig)

m011_exact_sd_orig <- lm(exact_sd ~ exp + stata + stat_skill , data = cri_team_orig)

m021_deviance_sd_orig <- lm(dev_sd ~ exp + stata + stat_skill , data = cri_team_orig)


# Unimportant criteria

# MEANS
m02_verif_mean_orig <- lm(verif_m ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team_orig)

m12_exact_mean_orig <- lm(exact_m ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team_orig)

m22_deviance_mean_orig <- lm(dev_m ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team_orig)

#SDs
m002_verif_sd_orig <- lm(verif_sd ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team_orig)

m012_exact_sd_orig <- lm(exact_sd ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team_orig)

m022_deviance_sd_orig <- lm(dev_sd ~ exp + stata + stat_skill  + degree + numinteam, data = cri_team_orig)


# Difficulty

# MEANS
m03_verif_mean_orig <- lm(verif_m ~ exp + stata + stat_skill + degree + numinteam + difficult + exp, data = cri_team_orig)

m13_exact_mean_orig <- lm(exact_m ~ exp + stata + stat_skill + degree + numinteam + difficult + exp, data = cri_team_orig)

m23_deviance_mean_orig <- lm(dev_m ~ exp + stata + stat_skill + degree + numinteam + difficult + exp, data = cri_team_orig)

#SDs
m003_verif_sd_orig <- lm(verif_sd ~ exp + stata + stat_skill  + difficult + exp, data = cri_team_orig)

m013_exact_sd_orig <- lm(exact_sd ~ exp + stata + stat_skill  + difficult + exp, data = cri_team_orig)

m023_deviance_sd_orig <- lm(dev_sd ~ exp + stata + stat_skill  + difficult + exp, data = cri_team_orig)

```

```{r cor}
cor <- select(cri_team, verif_m, exact_m, dev_m, stata, stat_skill, difficult)
cor1 <- cor(cor, use = "pairwise.complete.obs")

cor_orig <- select(cri_team_orig, verif_m, exact_m, dev_m, stata, stat_skill, difficult)
cor1_orig <- cor(cor_orig, use = "pairwise.complete.obs")

```


### Regression Tables Curated
```{r regtbl}
tab_model(m01_verif_mean, m02_verif_mean, m03_verif_mean, p.threshold = c(0.1, 0.05, 0.01), p.style = "star", show.ci = F)

tab_model(m11_exact_mean, m12_exact_mean, m13_exact_mean, p.threshold = c(0.1, 0.05, 0.01), p.style = "star", show.ci = F)


tab_model(m21_deviance_mean, m22_deviance_mean, m23_deviance_mean, p.threshold = c(0.1, 0.05, 0.01), p.style = "star", show.ci = F)

```

### Regression Tables Original
```{r regtbl_orig}
tab_model(m01_verif_mean_orig, m02_verif_mean_orig, m03_verif_mean_orig, p.threshold = c(0.1, 0.05, 0.01), p.style = "star", show.ci = F)


tab_model(m11_exact_mean_orig, m12_exact_mean_orig, m13_exact_mean_orig, p.threshold = c(0.1, 0.05, 0.01), p.style = "star", show.ci = F)


tab_model(m21_deviance_mean_orig, m22_deviance_mean_orig, m23_deviance_mean_orig, p.threshold = c(0.1, 0.05, 0.01), p.style = "star", show.ci = F)

```

### Indirect Effects

There are clearly effects of these through perceived difficulty

```{r diff_dv}
m1_diff <- lm(difficult ~ stata + stat_skill, data = cri_team)

m1_diff_orig <- lm(difficult ~ stata + stat_skill, data = cri_team_orig)

tab_model(m1_diff, m1_diff_orig, p.threshold = c(0.1, 0.05, 0.01), p.style = "star", show.ci = F)
```

### SEM

```{r sem}
m1 <- 'verif_m ~ exp + stata + stat_skill'
m2 <- 'verif_m ~ exp + stata + stat_skill + difficult'
m3 <- 'verif_m ~ exp + a2*stata + b2*stat_skill + m*difficult
       difficult ~ a1*stata + b1*stat_skill
       # total effects
         total_stata := a2 + (a1*m)
         total_stat_skill := b2 + (b1*m)
       # indirect effects
         indir_stata := a1*m
         indir_stat_skill := b1*m'

m1fit <- sem(m1, data = cri_team_orig)
m2fit <- sem(m2, data = cri_team_orig)
m3fit <- sem(m3, data = cri_team_orig)

m1out <- summary(m1fit, fit.measures = T)
m2out <- summary(m2fit, fit.measures = T)
m3out <- summary(m3fit, fit.measures = T)

# not really ideal for showing all features of the model
# lavaanPlot(model = m3fit, coefs = T, stand = T, sig = 1.00)

# m1out[["FIT"]][["aic"]]
```


### Plotting the model

What led to mistakes - lack of transparency, working in the same software as the original - some kind of procedural similarity that is unique to softwares. 

Show all three models and their fit statistics, then plot the direct and indirect effects.

```{r}


```


### Predict routine v. non-routine


